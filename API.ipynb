{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc9WaDXPOxOf",
    "outputId": "b4bd460c-04a4-4aa6-89e6-d6b28e1c2955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install mediapipe flask flask-cors pyngrok sentence-transformers joblib\n",
    "\n",
    "# Flask and CORS\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Ngrok (for tunneling in Colab/Notebook)\n",
    "from pyngrok import ngrok, conf\n",
    "conf.get_default().auth_token = \"Your_Authentication_Token\"\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# MediaPipe (for pose/landmark detection)\n",
    "import mediapipe as mp\n",
    "\n",
    "# Deep learning model loading\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Sentence transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Colab utilities\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "TiSFMeM3-ffL",
    "outputId": "9f065f65-1297-4e7b-bf57-bc4390ec9b6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4e80fcb3-9142-48e0-82b5-f51bf56f3714\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-4e80fcb3-9142-48e0-82b5-f51bf56f3714\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving anger_encoder.pkl to anger_encoder (1).pkl\n",
      "Saving anger_model.pkl to anger_model (1).pkl\n",
      "Saving anxiety_encoder.pkl to anxiety_encoder (1).pkl\n",
      "Saving anxiety_model.pkl to anxiety_model (1).pkl\n",
      "Saving confusion_encoder.pkl to confusion_encoder (1).pkl\n",
      "Saving confusion_model.pkl to confusion_model (1).pkl\n",
      "Saving emotion_encoder.pkl to emotion_encoder (1).pkl\n",
      "Saving emotion_model.pkl to emotion_model (1).pkl\n",
      "Saving guilt_encoder.pkl to guilt_encoder (1).pkl\n",
      "Saving guilt_model.pkl to guilt_model (1).pkl\n",
      "Saving hope_encoder.pkl to hope_encoder (1).pkl\n",
      "Saving hope_model.pkl to hope_model (1).pkl\n",
      "Saving joy_encoder.pkl to joy_encoder (1).pkl\n",
      "Saving joy_model.pkl to joy_model (1).pkl\n",
      "Saving sadness_encoder.pkl to sadness_encoder (1).pkl\n",
      "Saving sadness_model.pkl to sadness_model (1).pkl\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j6pP4WS0pnwy"
   },
   "outputs": [],
   "source": [
    "# Initialize mediapipe pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Helper function\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba, bc = a - b, c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Komd4K0mqNLk"
   },
   "outputs": [],
   "source": [
    "# Takbir (Raising hands near ears/shoulders)\n",
    "def check_takbir(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=True) as pose:\n",
    "        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            return False, \"No person detected\"\n",
    "\n",
    "        lm = results.pose_landmarks.landmark\n",
    "\n",
    "        rw = lm[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "        lw = lm[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "        re = lm[mp_pose.PoseLandmark.RIGHT_EAR.value]\n",
    "        le = lm[mp_pose.PoseLandmark.LEFT_EAR.value]\n",
    "        rs = lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "        ls = lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "\n",
    "        rw_px = np.array([rw.x * w, rw.y * h])\n",
    "        lw_px = np.array([lw.x * w, lw.y * h])\n",
    "        re_px = np.array([re.x * w, re.y * h])\n",
    "        le_px = np.array([le.x * w, le.y * h])\n",
    "        rs_px = np.array([rs.x * w, rs.y * h])\n",
    "        ls_px = np.array([ls.x * w, ls.y * h])\n",
    "\n",
    "        right_hand_ok = min(rs_px[1], re_px[1]) - 30 < rw_px[1] < max(rs_px[1], re_px[1]) + 30\n",
    "        left_hand_ok  = min(ls_px[1], le_px[1]) - 30 < lw_px[1] < max(ls_px[1], le_px[1]) + 30\n",
    "\n",
    "        right_near_head = abs(rw_px[0] - re_px[0]) < 80\n",
    "        left_near_head  = abs(lw_px[0] - le_px[0]) < 80\n",
    "\n",
    "        if right_hand_ok and left_hand_ok and right_near_head and left_near_head:\n",
    "            return True, \"Takbir correct.\"\n",
    "        else:\n",
    "            return False, \"Raise both hands near ears/shoulders for Takbir.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NPVi9W9aqZtR"
   },
   "outputs": [],
   "source": [
    "# Ruku (Bowing)\n",
    "def check_ruku(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return False, \"No person detected\"\n",
    "\n",
    "    lm = results.pose_landmarks.landmark\n",
    "\n",
    "    left_shoulder = [lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                     lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "    left_hip = [lm[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                lm[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "    left_knee = [lm[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                 lm[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "\n",
    "    r_hip, l_hip = lm[mp_pose.PoseLandmark.RIGHT_HIP.value], lm[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    r_knee, l_knee = lm[mp_pose.PoseLandmark.RIGHT_KNEE.value], lm[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "    r_ankle, l_ankle = lm[mp_pose.PoseLandmark.RIGHT_ANKLE.value], lm[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "\n",
    "    hips_y = (r_hip.y + l_hip.y) / 2\n",
    "    knees_y = (r_knee.y + l_knee.y) / 2\n",
    "    ankles_y = (r_ankle.y + l_ankle.y) / 2\n",
    "\n",
    "    standing = hips_y < knees_y < ankles_y\n",
    "    if not standing:\n",
    "        return False, \"Stand upright with legs straight before bending into Ruku.\"\n",
    "\n",
    "    angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "    if not (70 <= angle <= 110):\n",
    "        return False, \"Bend your back so it is roughly horizontal in Ruku.\"\n",
    "\n",
    "    return True, \"Ruku correct: back straight and posture maintained.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "u9Mo_I8mqgmW"
   },
   "outputs": [],
   "source": [
    "# Standing hand placement (hands on chest)\n",
    "def check_qiyam_hands(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return False, \"No person detected\"\n",
    "\n",
    "    lm = results.pose_landmarks.landmark\n",
    "    rw, lw = lm[mp_pose.PoseLandmark.RIGHT_WRIST], lm[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "    rs, ls = lm[mp_pose.PoseLandmark.RIGHT_SHOULDER], lm[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "\n",
    "    chest_y = (rs.y + ls.y) / 2\n",
    "    hands_avg_y = (rw.y + lw.y) / 2\n",
    "    hands_avg_x = (rw.x + lw.x) / 2\n",
    "    shoulders_avg_x = (rs.x + ls.x) / 2\n",
    "\n",
    "    if abs(hands_avg_y - chest_y) >= 0.1 or abs(hands_avg_x - shoulders_avg_x) >= 0.05:\n",
    "        return False, \"Place your hands in the center of your chest.\"\n",
    "\n",
    "    if rw.y >= lw.y:\n",
    "        return False, \"Place your right hand on top of your left hand.\"\n",
    "\n",
    "    r_hip, l_hip = lm[mp_pose.PoseLandmark.RIGHT_HIP], lm[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "    r_knee, l_knee = lm[mp_pose.PoseLandmark.RIGHT_KNEE], lm[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "    r_ankle, l_ankle = lm[mp_pose.PoseLandmark.RIGHT_ANKLE], lm[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "\n",
    "    hips_y = (r_hip.y + l_hip.y) / 2\n",
    "    knees_y = (r_knee.y + l_knee.y) / 2\n",
    "    ankles_y = (r_ankle.y + l_ankle.y) / 2\n",
    "\n",
    "    if not (hips_y < knees_y < ankles_y):\n",
    "        return False, \"Stand upright with your legs straight.\"\n",
    "\n",
    "    nose, left_eye, right_eye = lm[mp_pose.PoseLandmark.NOSE], lm[mp_pose.PoseLandmark.LEFT_EYE], lm[mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "    if ((left_eye.y + right_eye.y) / 2) <= nose.y:\n",
    "        return False, \"Tilt your head downward, looking where you will do sujood.\"\n",
    "\n",
    "    return True, \"Correct Qiyam posture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "soe_x2yGqsYk"
   },
   "outputs": [],
   "source": [
    "# Sitting between sujood (back view)\n",
    "def check_sitting_back(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    with mp_pose.Pose(static_image_mode=True) as pose:\n",
    "        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            return False, \"No person detected\"\n",
    "\n",
    "        lm = results.pose_landmarks.landmark\n",
    "\n",
    "        hip_left   = [lm[mp_pose.PoseLandmark.LEFT_HIP.value].x, lm[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "        knee_left  = [lm[mp_pose.PoseLandmark.LEFT_KNEE.value].x, lm[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "        ankle_left = [lm[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, lm[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "\n",
    "        hip_right   = [lm[mp_pose.PoseLandmark.RIGHT_HIP.value].x, lm[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "        knee_right  = [lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "        ankle_right = [lm[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, lm[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "        left_leg_angle  = calculate_angle(hip_left, knee_left, ankle_left)\n",
    "        right_leg_angle = calculate_angle(hip_right, knee_right, ankle_right)\n",
    "\n",
    "        if left_leg_angle >= 100:\n",
    "            return False, \"Fold your left leg more so the knee is closer to the floor.\"\n",
    "        if not (70 < right_leg_angle < 130):\n",
    "            return False, \"Adjust your right leg to be upright (around 90 degrees).\"\n",
    "\n",
    "        return True, \"Back/side view: Sitting posture correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "T-mNe3ooqwPa"
   },
   "outputs": [],
   "source": [
    "# Sitting between sujood (front view)\n",
    "def check_sitting_front(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "    with mp_pose.Pose(static_image_mode=True) as pose:\n",
    "        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            return False, \"No person detected\"\n",
    "\n",
    "        lm = results.pose_landmarks.landmark\n",
    "\n",
    "        rs = [lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "        ls = [lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        rh = [lm[mp_pose.PoseLandmark.RIGHT_HIP.value].x, lm[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "        lh = [lm[mp_pose.PoseLandmark.LEFT_HIP.value].x, lm[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "\n",
    "        torso_upright = abs((rs[1]+ls[1])/2 - (rh[1]+lh[1])/2) < 0.15\n",
    "        if not torso_upright:\n",
    "            return False, \"Keep your torso upright while sitting between sujud.\"\n",
    "\n",
    "        wrist_left  = np.array([lm[mp_pose.PoseLandmark.LEFT_WRIST.value].x*w, lm[mp_pose.PoseLandmark.LEFT_WRIST.value].y*h])\n",
    "        wrist_right = np.array([lm[mp_pose.PoseLandmark.RIGHT_WRIST.value].x*w, lm[mp_pose.PoseLandmark.RIGHT_WRIST.value].y*h])\n",
    "        knee_left   = np.array([lm[mp_pose.PoseLandmark.LEFT_KNEE.value].x*w, lm[mp_pose.PoseLandmark.LEFT_KNEE.value].y*h])\n",
    "        knee_right  = np.array([lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].x*w, lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].y*h])\n",
    "\n",
    "        if np.linalg.norm(wrist_left - knee_left) >= 0.1*h:\n",
    "            return False, \"Place your left hand on your left thigh.\"\n",
    "        if np.linalg.norm(wrist_right - knee_right) >= 0.1*h:\n",
    "            return False, \"Place your right hand on your right thigh.\"\n",
    "\n",
    "        return True, \"Front view: Sitting posture correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lN6aTyGyq4Ib"
   },
   "outputs": [],
   "source": [
    "# Sujood\n",
    "def check_sujood(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return False, \"No person detected\"\n",
    "\n",
    "    lm = results.pose_landmarks.landmark\n",
    "\n",
    "    left_knee = [lm[mp_pose.PoseLandmark.LEFT_KNEE.value].x, lm[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "    right_knee = [lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, lm[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "    nose_y = lm[mp_pose.PoseLandmark.NOSE.value].y\n",
    "    knee_y = (left_knee[1] + right_knee[1]) / 2\n",
    "\n",
    "    left_elbow, right_elbow = lm[mp_pose.PoseLandmark.LEFT_ELBOW.value], lm[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "    left_wrist, right_wrist = lm[mp_pose.PoseLandmark.LEFT_WRIST.value], lm[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "\n",
    "    if not (nose_y > knee_y):\n",
    "        return False, \"Lower your head so it touches the ground in Sujood.\"\n",
    "    if not ((left_elbow.y < left_wrist.y - 0.05) and (right_elbow.y < right_wrist.y - 0.05)):\n",
    "        return False, \"Lift your elbows off the ground during Sujood.\"\n",
    "\n",
    "    return True, \"Sujood correct: head down, elbows lifted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWeXzIdaKz5W",
    "outputId": "08a48373-5fbd-4eae-e433-504fbd01844b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API is available at: NgrokTunnel: \"https://ba13f6b9023b.ngrok-free.app\" -> \"http://localhost:5000\"\n",
      "Use /classify for posture and /predict for emotion\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load Models\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "emotion_model = joblib.load(\"emotion_model.pkl\")\n",
    "emotion_encoder = joblib.load(\"emotion_encoder.pkl\")\n",
    "\n",
    "subcategory_models = {\n",
    "    \"sadness\": {\"model\": joblib.load(\"sadness_model.pkl\"), \"encoder\": joblib.load(\"sadness_encoder.pkl\")},\n",
    "    \"anxiety\": {\"model\": joblib.load(\"anxiety_model.pkl\"), \"encoder\": joblib.load(\"anxiety_encoder.pkl\")},\n",
    "    \"anger\": {\"model\": joblib.load(\"anger_model.pkl\"), \"encoder\": joblib.load(\"anger_encoder.pkl\")},\n",
    "    \"joy\": {\"model\": joblib.load(\"joy_model.pkl\"), \"encoder\": joblib.load(\"joy_encoder.pkl\")},\n",
    "    \"confusion\": {\"model\": joblib.load(\"confusion_model.pkl\"), \"encoder\": joblib.load(\"confusion_encoder.pkl\")},\n",
    "    \"hope\": {\"model\": joblib.load(\"hope_model.pkl\"), \"encoder\": joblib.load(\"hope_encoder.pkl\")},\n",
    "    \"guilt\": {\"model\": joblib.load(\"guilt_model.pkl\"), \"encoder\": joblib.load(\"guilt_encoder.pkl\")},\n",
    "}\n",
    "\n",
    "# Text emotion prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    text = data.get(\"text\", \"\")\n",
    "\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "    # Convert input to embedding\n",
    "    embedding = embedder.encode([text])\n",
    "\n",
    "    # Predict main emotion\n",
    "    emotion_pred = emotion_model.predict(embedding)\n",
    "    emotion = emotion_encoder.inverse_transform(emotion_pred)[0]\n",
    "\n",
    "    # Subcategory classification\n",
    "    if emotion in subcategory_models:\n",
    "        sub_model = subcategory_models[emotion][\"model\"]\n",
    "        sub_encoder = subcategory_models[emotion][\"encoder\"]\n",
    "        sub_pred = sub_model.predict(embedding)\n",
    "        subcategory = sub_encoder.inverse_transform(sub_pred)[0]\n",
    "    else:\n",
    "        subcategory = \"unknown\"\n",
    "\n",
    "    return jsonify({\"emotion\": emotion, \"subcategory\": subcategory})\n",
    "\n",
    "# Image classification\n",
    "@app.route('/classify', methods=['POST'])\n",
    "def classify():\n",
    "    position = request.form.get(\"position\")\n",
    "\n",
    "    if \"image\" not in request.files:\n",
    "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
    "\n",
    "    image_file = request.files[\"image\"]\n",
    "    image_path = os.path.join(\"/content\", image_file.filename)\n",
    "    image_file.save(image_path)\n",
    "\n",
    "    # Call functions based on position\n",
    "    if position == \"qiyam_hands\":\n",
    "        result, message = check_qiyam_hands(image_path)\n",
    "    elif position == \"ruku\":\n",
    "        result, message = check_ruku(image_path)\n",
    "    elif position == \"sujood\":\n",
    "        result, message = check_sujood(image_path)\n",
    "    elif position == \"sitting_back\":\n",
    "        result, message = check_sitting_back(image_path)\n",
    "    elif position == \"sitting_front\":\n",
    "        result, message = check_sitting_front(image_path)\n",
    "    elif position == \"takbir\":\n",
    "        result, message = check_takbir(image_path)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Unknown position\"}), 400\n",
    "\n",
    "    return jsonify({\"result\": result, \"message\": message})\n",
    "\n",
    "\n",
    "# Run App\n",
    "if __name__ == \"__main__\":\n",
    "    # Just one tunnel for everything\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(\"API is available at:\", public_url)\n",
    "    print(\"Use /classify for posture and /predict for emotion\")\n",
    "\n",
    "    app.run(port=5000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
